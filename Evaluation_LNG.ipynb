{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "creates evaluation csv"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d36d96a8c8d6418"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-20T09:27:53.471839900Z",
     "start_time": "2025-07-20T09:27:47.816937Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of AD identifiers\n",
    "ad_list = [\n",
    "    'AD_2018-0289R1_1',\n",
    "    'AD_2019-0056_1',\n",
    "    'AD_2019-0173_1',\n",
    "    'AD_2020-0030_1',\n",
    "    'AD_2020-0053_1',\n",
    "    'AD_2020-0118_1',\n",
    "    'AD_2020-0148_1',\n",
    "    'AD_2020-0219_1',\n",
    "    'AD_2020-0250_1',\n",
    "    'AD_2021-0172_2',\n",
    "    'AD_2021-0236_1',\n",
    "    'AD_2021-0256_1',\n",
    "    'AD_2021-0279_2',\n",
    "    'AD_2022-0032R1_1',\n",
    "    'AD_2022-0115_2',\n",
    "    'AD_2022-0185_1'\n",
    "]\n",
    "\n",
    "# Create a DataFrame template\n",
    "df = pd.DataFrame({\n",
    "    'AD_Number': ad_list,\n",
    "    'Models': [''] * len(ad_list),\n",
    "    'Conditions': [''] * len(ad_list),\n",
    "    'Affected_Parts': [''] * len(ad_list),\n",
    "    'Total_Extracted': [''] * len(ad_list),\n",
    "    'Time_Parsed_sec': [''] * len(ad_list)\n",
    "})\n",
    "\n",
    "# Display the DataFrame to the user\n",
    "#import ace_tools as tools; tools.display_dataframe_to_user(name=\"AD Extraction Metrics\", dataframe=df)\n",
    "\n",
    "# Optionally, save to CSV\n",
    "df.to_csv(r'C:\\Users\\zdrop\\OneDrive - TU Wien\\MASTER THESIS\\ADs\\A320\\directives\\sample dataset\\golden_data_llm_without\\ad_extraction_metrics_template.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "finds missing model numbers in llm without guidance output"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80d30dcc6b6e78d4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching Models: []\n",
      "Missing Models: ['A319-151N', 'A319-153N', 'A319-171N', 'A320-251N', 'A320-252N', 'A320-253N', 'A320-271N', 'A320-272N', 'A320-273N', 'A321-251N', 'A321-251NX', 'A321-252N', 'A321-252NX', 'A321-253N', 'A321-253NX', 'A321-271N', 'A321-271NX', 'A321-272N', 'A321-272NX']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def find_matching_and_missing_models(json_file1_path, json_file2_path):\n",
    "    \"\"\"\n",
    "    Compares aircraft models between two JSON files (specified by paths)\n",
    "    and identifies matching and missing models.\n",
    "\n",
    "    Args:\n",
    "        json_file1_path (str): The path to the first JSON file (ground truth).\n",
    "        json_file2_path (str): The path to the second JSON file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two lists:\n",
    "               - matching_models (list): Models found in both files.\n",
    "               - missing_models (list): Models present in file1 but missing in file2.\n",
    "    \"\"\"\n",
    "\n",
    "    ground_truth_models = set()\n",
    "    extracted_models_from_file2 = set()\n",
    "\n",
    "    # Load data from the first JSON file\n",
    "    try:\n",
    "        with open(json_file1_path, 'r') as f:\n",
    "            json_file1_data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {json_file1_path}\")\n",
    "        return [], []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Could not decode JSON from {json_file1_path}\")\n",
    "        return [], []\n",
    "\n",
    "    # Load data from the second JSON file\n",
    "    try:\n",
    "        with open(json_file2_path, 'r') as f:\n",
    "            json_file2_data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {json_file2_path}\")\n",
    "        return [], []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Could not decode JSON from {json_file2_path}\")\n",
    "        return [], []\n",
    "\n",
    "    # Extract models from the first JSON file (ground truth)\n",
    "    if \"applicability_groups\" in json_file1_data:\n",
    "        for group in json_file1_data[\"applicability_groups\"]:\n",
    "            if \"models\" in group:\n",
    "                ground_truth_models.update(group[\"models\"])\n",
    "\n",
    "    # Extract models from the raw_output of the second JSON file\n",
    "    if \"raw_output\" in json_file2_data:\n",
    "        raw_output = json_file2_data[\"raw_output\"]\n",
    "        # Regex to find aircraft models (e.g., A318-111, A319, A320)\n",
    "        model_pattern = re.compile(r'\\b[A]\\d{3}(?:-\\d{3})?\\b')\n",
    "        found_models = model_pattern.findall(raw_output)\n",
    "        extracted_models_from_file2.update(found_models)\n",
    "\n",
    "    matching_models = sorted(list(ground_truth_models.intersection(extracted_models_from_file2)))\n",
    "    missing_models = sorted(list(ground_truth_models.difference(extracted_models_from_file2)))\n",
    "\n",
    "    return matching_models, missing_models\n",
    "\n",
    "# Define file paths\n",
    "file1_path = r\"C:\\Users\\zdrop\\OneDrive - TU Wien\\MASTER THESIS\\ADs\\A320\\directives\\sample dataset\\llm_without guidance\\golden_data_llm_without\\AD_2022-0185_1_c.json\"\n",
    "file2_path = r\"C:\\Users\\zdrop\\OneDrive - TU Wien\\MASTER THESIS\\ADs\\A320\\directives\\sample dataset\\llm_without guidance\\golden_data_llm_without\\AD_2022-0185_1_lng.json\"\n",
    "\n",
    "\n",
    "# Call the function with file paths\n",
    "matching_models, missing_models = find_matching_and_missing_models(file1_path, file2_path)\n",
    "\n",
    "print(\"Matching Models:\", matching_models)\n",
    "print(\"Missing Models:\", missing_models)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-20T12:14:05.268390900Z",
     "start_time": "2025-07-20T12:14:05.126355800Z"
    }
   },
   "id": "862db60e40e07397",
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "metrics for evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4240126b81f34bba"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Quantitative Metrics Results ---\n",
      "\n",
      "Parsing Time Seconds:\n",
      "  Mean: 9.435\n",
      "  Median: 8.980\n",
      "  Std Dev: 2.419\n",
      "  Min: 6.180\n",
      "  Max: 26.600\n",
      "\n",
      "Raw Text Character Length:\n",
      "  Mean: 1504.720\n",
      "  Median: 1272.000\n",
      "  Std Dev: 890.388\n",
      "  Min: 305.000\n",
      "  Max: 5065.000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def analyze_ad_files(file_contents):\n",
    "    \"\"\"\n",
    "    Analyzes a list of AD file contents to extract quantitative metrics.\n",
    "\n",
    "    Args:\n",
    "        file_contents (list): A list of dictionaries, where each dictionary\n",
    "                              represents the content of an AD JSON file.\n",
    "                              Each dictionary is expected to have 'raw_output'\n",
    "                              and 'processing_time_seconds' keys.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing calculated statistics for parsing time\n",
    "              and raw text character length.\n",
    "    \"\"\"\n",
    "    parsing_times = []\n",
    "    raw_text_lengths = []\n",
    "\n",
    "    if not file_contents:\n",
    "        print(\"No file contents provided for analysis.\")\n",
    "        return {}\n",
    "\n",
    "    for file_data in file_contents:\n",
    "        try:\n",
    "            # Extract processing time\n",
    "            if 'processing_time_seconds' in file_data:\n",
    "                parsing_times.append(file_data['processing_time_seconds'])\n",
    "            else:\n",
    "                print(f\"Warning: 'processing_time_seconds' not found in a file.\")\n",
    "\n",
    "            # Extract raw text length\n",
    "            if 'raw_output' in file_data and isinstance(file_data['raw_output'], str):\n",
    "                raw_text_lengths.append(len(file_data['raw_output']))\n",
    "            else:\n",
    "                print(f\"Warning: 'raw_output' not found or not a string in a file.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing a file: {e}\")\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    if parsing_times:\n",
    "        metrics['parsing_time_seconds'] = {\n",
    "            'mean': np.mean(parsing_times),\n",
    "            'median': np.median(parsing_times),\n",
    "            'std_dev': np.std(parsing_times),\n",
    "            'min': np.min(parsing_times),\n",
    "            'max': np.max(parsing_times)\n",
    "        }\n",
    "    else:\n",
    "        print(\"No valid parsing times found to calculate statistics.\")\n",
    "\n",
    "    if raw_text_lengths:\n",
    "        metrics['raw_text_character_length'] = {\n",
    "            'mean': np.mean(raw_text_lengths),\n",
    "            'median': np.median(raw_text_lengths),\n",
    "            'std_dev': np.std(raw_text_lengths),\n",
    "            'min': np.min(raw_text_lengths),\n",
    "            'max': np.max(raw_text_lengths)\n",
    "        }\n",
    "    else:\n",
    "        print(\"No valid raw text lengths found to calculate statistics.\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# --- Example Usage (replace with your actual file loading logic) ---\n",
    "# In a real scenario, you would load your 100 JSON files here.\n",
    "# For demonstration, we'll use the provided snippets.\n",
    "\n",
    "# Simulate loading file contents from the user's uploaded files\n",
    "# In your actual environment, you'd iterate through your 100 files\n",
    "# and parse each JSON into a dictionary.\n",
    "\n",
    "# To load all your files, you would do something like this (assuming they are in a 'data' folder):\n",
    "all_file_contents = []\n",
    "data_folder = r'C:\\Users\\zdrop\\OneDrive - TU Wien\\MASTER THESIS\\ADs\\A320\\directives\\sample dataset\\llm_without guidance\\dataset_llm_without' # Replace with the actual path to your files\n",
    "for filename in os.listdir(data_folder):\n",
    "    if filename.endswith('.json'):\n",
    "        filepath = os.path.join(data_folder, filename)\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            all_file_contents.append(data)\n",
    "\n",
    "# Run the analysis\n",
    "# For your 100 files, replace `example_file_contents` with `all_file_contents`\n",
    "results = analyze_ad_files(all_file_contents)\n",
    "print(\"\\n--- Quantitative Metrics Results ---\")\n",
    "for metric_type, stats in results.items():\n",
    "    print(f\"\\n{metric_type.replace('_', ' ').title()}:\")\n",
    "    for stat_name, value in stats.items():\n",
    "        print(f\"  {stat_name.replace('_', ' ').title()}: {value:.3f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-24T10:06:35.694773600Z",
     "start_time": "2025-07-24T10:06:31.955870600Z"
    }
   },
   "id": "73d1f4b31a09e1b1",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "histograms for evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65a7c03c186579e5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt # Import matplotlib for plotting\n",
    "\n",
    "def analyze_ad_files(file_contents):\n",
    "    \"\"\"\n",
    "    Analyzes a list of AD file contents to extract quantitative metrics.\n",
    "\n",
    "    Args:\n",
    "        file_contents (list): A list of dictionaries, where each dictionary\n",
    "                              represents the content of an AD JSON file.\n",
    "                              Each dictionary is expected to have 'raw_output'\n",
    "                              and 'processing_time_seconds' keys.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "               - dict: Calculated statistics for parsing time and raw text character length.\n",
    "               - list: List of parsing times.\n",
    "               - list: List of raw text character lengths.\n",
    "    \"\"\"\n",
    "    parsing_times = []\n",
    "    raw_text_lengths = []\n",
    "\n",
    "    if not file_contents:\n",
    "        print(\"No file contents provided for analysis.\")\n",
    "        return {}, [], []\n",
    "\n",
    "    for file_data in file_contents:\n",
    "        try:\n",
    "            # Extract processing time\n",
    "            if 'processing_time_seconds' in file_data:\n",
    "                parsing_times.append(file_data['processing_time_seconds'])\n",
    "            else:\n",
    "                print(f\"Warning: 'processing_time_seconds' not found in a file.\")\n",
    "\n",
    "            # Extract raw text length\n",
    "            if 'raw_output' in file_data and isinstance(file_data['raw_output'], str):\n",
    "                raw_text_lengths.append(len(file_data['raw_output']))\n",
    "            else:\n",
    "                print(f\"Warning: 'raw_output' not found or not a string in a file.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing a file: {e}\")\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    if parsing_times:\n",
    "        metrics['parsing_time_seconds'] = {\n",
    "            'mean': np.mean(parsing_times),\n",
    "            'median': np.median(parsing_times),\n",
    "            'std_dev': np.std(parsing_times),\n",
    "            'min': np.min(parsing_times),\n",
    "            'max': np.max(parsing_times)\n",
    "        }\n",
    "    else:\n",
    "        print(\"No valid parsing times found to calculate statistics.\")\n",
    "\n",
    "    if raw_text_lengths:\n",
    "        metrics['raw_text_character_length'] = {\n",
    "            'mean': np.mean(raw_text_lengths),\n",
    "            'median': np.median(raw_text_lengths),\n",
    "            'std_dev': np.std(raw_text_lengths),\n",
    "            'min': np.min(raw_text_lengths),\n",
    "            'max': np.max(raw_text_lengths)\n",
    "        }\n",
    "    else:\n",
    "        print(\"No valid raw text lengths found to calculate statistics.\")\n",
    "\n",
    "    return metrics, parsing_times, raw_text_lengths\n",
    "\n",
    "def plot_histograms(parsing_times, raw_text_lengths):\n",
    "    \"\"\"\n",
    "    Generates and displays histograms for processing times and raw text lengths.\n",
    "\n",
    "    Args:\n",
    "        parsing_times (list): A list of processing times in seconds.\n",
    "        raw_text_lengths (list): A list of raw text character lengths.\n",
    "    \"\"\"\n",
    "    if parsing_times and raw_text_lengths:\n",
    "        plt.figure(figsize=(14, 6), facecolor='white') # Create a figure with a good size for two plots\n",
    "\n",
    "        # Histogram for Processing Time\n",
    "        plt.subplot(1, 2, 1) # 1 row, 2 columns, 1st plot\n",
    "        plt.hist(parsing_times, bins=20, edgecolor='black', color='skyblue') # Use 20 bins for good resolution\n",
    "        plt.title('Distribution of LLM Processing Times', fontsize=14)\n",
    "        plt.xlabel('Processing Time (seconds)', fontsize=12)\n",
    "        plt.ylabel('Number of Files', fontsize=12)\n",
    "        plt.grid(axis='y', alpha=0.75) # Add a grid for readability\n",
    "\n",
    "        # Histogram for Raw Text Character Length\n",
    "        plt.subplot(1, 2, 2) # 1 row, 2 columns, 2nd plot\n",
    "        plt.hist(raw_text_lengths, bins=20, edgecolor='black', color='lightcoral') # Use 20 bins\n",
    "        plt.title('Distribution of Raw Text Character Lengths', fontsize=14)\n",
    "        plt.xlabel('Raw Text Length (characters)', fontsize=12)\n",
    "        plt.ylabel('Number of Files', fontsize=12)\n",
    "        plt.grid(axis='y', alpha=0.75) # Add a grid for readability\n",
    "        \n",
    "        plt.tight_layout() # Adjust layout to prevent overlapping titles/labels\n",
    "        plt.show() # Display the plots\n",
    "    else:\n",
    "        print(\"\\nCannot generate histograms: Not enough data for plotting.\")\n",
    "\n",
    "# --- File Loading Logic (from your provided code) ---\n",
    "all_file_contents = []\n",
    "# IMPORTANT: Replace this with the actual path to your dataset\n",
    "data_folder = r'C:\\Users\\zdrop\\OneDrive - TU Wien\\MASTER THESIS\\ADs\\A320\\directives\\sample dataset\\llm_without guidance\\dataset_llm_without'\n",
    "\n",
    "# Check if the data folder exists to prevent errors\n",
    "if not os.path.exists(data_folder):\n",
    "    print(f\"Error: The specified data folder does not exist: {data_folder}\")\n",
    "    print(\"Please ensure the path is correct and accessible.\")\n",
    "else:\n",
    "    for filename in os.listdir(data_folder):\n",
    "        if filename.endswith('.json'):\n",
    "            filepath = os.path.join(data_folder, filename)\n",
    "            try:\n",
    "                with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                    all_file_contents.append(data)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON from {filename}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"An unexpected error occurred while reading {filename}: {e}\")\n",
    "\n",
    "# Run the analysis to get metrics and the lists of data for plotting\n",
    "metrics_results, parsing_times, raw_text_lengths = analyze_ad_files(all_file_contents)\n",
    "\n",
    "print(\"\\n--- Quantitative Metrics Results ---\")\n",
    "for metric_type, stats in metrics_results.items():\n",
    "    print(f\"\\n{metric_type.replace('_', ' ').title()}:\")\n",
    "    for stat_name, value in stats.items():\n",
    "        print(f\"  {stat_name.replace('_', ' ').title()}: {value:.3f}\")\n",
    "\n",
    "# Call the new plotting function\n",
    "plot_histograms(parsing_times, raw_text_lengths)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d23e2767d13d43c",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Errors stats"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1e3601f38996b0d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Extraction Accuracy Metrics Results ---\n",
      "\n",
      "Ad Number Score:\n",
      "  Mean: 0.375\n",
      "  Median: 0.000\n",
      "  Std Dev: 0.500\n",
      "  Min: 0.000\n",
      "  Max: 1.000\n",
      "\n",
      "Models Score:\n",
      "  Mean: 0.438\n",
      "  Median: 0.000\n",
      "  Std Dev: 0.512\n",
      "  Min: 0.000\n",
      "  Max: 1.000\n",
      "\n",
      "Conditions Score:\n",
      "  Mean: 0.875\n",
      "  Median: 1.000\n",
      "  Std Dev: 0.289\n",
      "  Min: 0.000\n",
      "  Max: 1.000\n",
      "\n",
      "Affected Parts Score:\n",
      "  Mean: 0.500\n",
      "  Median: 0.500\n",
      "  Std Dev: 0.483\n",
      "  Min: 0.000\n",
      "  Max: 1.000\n",
      "\n",
      "Total Accuracy Score:\n",
      "  Mean: 2.188\n",
      "  Median: 2.000\n",
      "  Std Dev: 0.892\n",
      "  Min: 1.000\n",
      "  Max: 4.000\n",
      "\n",
      "--- Error Rate Metrics Results ---\n",
      "\n",
      "Error Missing Data:\n",
      "  Incidence Rate (Proportion of Files with Error): 0.562\n",
      "  Count (Number of Files with Error): 9\n",
      "\n",
      "Error Redundant Information:\n",
      "  Incidence Rate (Proportion of Files with Error): 0.438\n",
      "  Count (Number of Files with Error): 7\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def analyze_csv_metrics(filepath):\n",
    "    \"\"\"\n",
    "    Analyzes the provided CSV file to extract statistics for accuracy and error metrics.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The full path to the ad_extraction_metrics_template.csv file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing calculated statistics for accuracy metrics\n",
    "              and error rate metrics.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(filepath)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file was not found at {filepath}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the CSV file: {e}\")\n",
    "        return {}\n",
    "\n",
    "    # --- Extraction Accuracy Metrics ---\n",
    "    # Columns representing accuracy scores\n",
    "    accuracy_cols = ['AD_number', 'Models', 'Conditions', 'Affected_Parts', 'Total_Accuracy']\n",
    "    accuracy_stats = {}\n",
    "    for col in accuracy_cols:\n",
    "        if col in df.columns:\n",
    "            accuracy_stats[col] = {\n",
    "                'mean': df[col].mean(),\n",
    "                'median': df[col].median(),\n",
    "                'std_dev': df[col].std(),\n",
    "                'min': df[col].min(),\n",
    "                'max': df[col].max()\n",
    "            }\n",
    "        else:\n",
    "            print(f\"Warning: Accuracy column '{col}' not found in CSV.\")\n",
    "\n",
    "    # --- Error Rate Metrics ---\n",
    "    # Columns representing binary error flags\n",
    "    error_cols = ['error_missing_data', 'error_redundant_information']\n",
    "    error_stats = {}\n",
    "    total_entries = len(df)\n",
    "\n",
    "    for col in error_cols:\n",
    "        if col in df.columns:\n",
    "            # For binary (0/1) columns, the mean is the proportion of '1's (i.e., incidence rate)\n",
    "            error_stats[col] = {\n",
    "                'incidence_rate': df[col].mean(), # Proportion of files with this error\n",
    "                'count': df[col].sum() # Total number of files with this error\n",
    "            }\n",
    "        else:\n",
    "            print(f\"Warning: Error column '{col}' not found in CSV.\")\n",
    "\n",
    "    return {\n",
    "        'extraction_accuracy_metrics': accuracy_stats,\n",
    "        'error_rate_metrics': error_stats\n",
    "    }\n",
    "\n",
    "# --- Example Usage ---\n",
    "# IMPORTANT: Replace this with the actual path to your ad_extraction_metrics_template.csv file\n",
    "csv_filepath = r'C:\\Users\\zdrop\\OneDrive - TU Wien\\MASTER THESIS\\ADs\\A320\\directives\\sample dataset\\llm_without guidance\\golden_data_llm_without\\ad_extraction_metrics_template.csv' # Assuming the CSV is in the same directory as your JSONs, or specify its exact path.\n",
    "\n",
    "# Check if the file exists before attempting to analyze\n",
    "if not os.path.exists(csv_filepath):\n",
    "    print(f\"Error: The CSV file does not exist at the specified path: {csv_filepath}\")\n",
    "    print(\"Please ensure the path is correct and accessible.\")\n",
    "else:\n",
    "    metrics_results = analyze_csv_metrics(csv_filepath)\n",
    "\n",
    "    if metrics_results:\n",
    "        print(\"\\n--- Extraction Accuracy Metrics Results ---\")\n",
    "        for metric, stats in metrics_results['extraction_accuracy_metrics'].items():\n",
    "            print(f\"\\n{metric.replace('_', ' ').title()} Score:\")\n",
    "            for stat_name, value in stats.items():\n",
    "                print(f\"  {stat_name.replace('_', ' ').title()}: {value:.3f}\")\n",
    "\n",
    "        print(\"\\n--- Error Rate Metrics Results ---\")\n",
    "        for metric, stats in metrics_results['error_rate_metrics'].items():\n",
    "            print(f\"\\n{metric.replace('_', ' ').title()}:\")\n",
    "            print(f\"  Incidence Rate (Proportion of Files with Error): {stats['incidence_rate']:.3f}\")\n",
    "            print(f\"  Count (Number of Files with Error): {int(stats['count'])}\")\n",
    "    else:\n",
    "        print(\"No metrics results to display.\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-24T14:10:47.832187900Z",
     "start_time": "2025-07-24T14:10:47.781024300Z"
    }
   },
   "id": "a584d91c769435fb",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d8ccc18bf91c37b2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
